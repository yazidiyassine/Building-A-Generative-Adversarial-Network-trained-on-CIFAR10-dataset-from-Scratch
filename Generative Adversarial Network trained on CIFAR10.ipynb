{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "swedish-carolina",
   "metadata": {},
   "source": [
    "# Build and train A Generative Adversarial Network\n",
    "\n",
    "In this notebook you'll find the entire code presented in the previous lessons. We once again use our beloved CIFAR10 dataset.\n",
    "\n",
    "We define a Generator and a Discriminator network, whic consist only of `Linear` and `Relu` layers.`Dropout` is also use to prevent overfitting. We then train them both using the process we described in the lesson. \n",
    "\n",
    "Feel free to play around, execute a few epochs and make sure that you understand everything you learned in this chapter.\n",
    "\n",
    "You can also try to alternate the network by adding more layers. Try adding a supervised pixel-wise loss (`torch.nn.L1Loss`) in the generator's output and Notice if the performance gets better. Are the images more realistic? What is the relationship between L1 loss and adversarial loss optimization? What are the best scalar terms to balance the two losses?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grateful-mouth",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "cwd = os.getcwd()\n",
    "#add CIFAR10 data in the environment\n",
    "sys.path.append(cwd + '/../cifar10') \n",
    "\n",
    "#Numpy is linear algebra lbrary\n",
    "import numpy as np\n",
    "# Matplotlib is a visualizations library \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision import utils\n",
    "from torchvision import transforms\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "#CIFAR10 is a custom Dataloader that loads a subset ofthe data from a local folder\n",
    "from Cifar10Dataloader import CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dated-support",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=4\n",
    "\n",
    "def load_data():\n",
    "    \n",
    "    #convert the images to tensor and normalized them\n",
    "    transform = transforms.Compose([\n",
    "         transforms.ToTensor(),\n",
    "         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ])\n",
    "\n",
    "    trainset = CIFAR10(root='../cifar10',  transform=transform)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                              shuffle=False, num_workers=1)\n",
    "    return trainloader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preceding-console",
   "metadata": {},
   "source": [
    "## Utility functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diagnostic-pathology",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def vectors_to_images(vectors):\n",
    "    return vectors.view(vectors.size(0), 3, 32, 32)\n",
    "\n",
    "def noise(size):\n",
    "    '''\n",
    "    Generates a 1-d vector of gaussian sampled random values\n",
    "    '''\n",
    "    n = Variable(torch.randn(size, 100))\n",
    "    return n\n",
    "\n",
    "def log_images(images, num_images, format='NCHW', normalize=True):\n",
    "    '''\n",
    "    input images are expected in format (NCHW)\n",
    "    '''\n",
    "    if type(images) == np.ndarray:\n",
    "        images = torch.from_numpy(images)\n",
    "\n",
    "    if format == 'NHWC':\n",
    "        images = images.transpose(1, 3)\n",
    "\n",
    "    \n",
    "\n",
    "    # Make horizontal grid from image tensor\n",
    "    horizontal_grid = vutils.make_grid(\n",
    "        images, normalize=normalize, scale_each=True)\n",
    "    # Make vertical grid from image tensor\n",
    "    nrows = int(np.sqrt(num_images))\n",
    "    grid = vutils.make_grid(\n",
    "        images, nrow=nrows, normalize=True, scale_each=True)\n",
    "\n",
    "    \n",
    "    fig = plt.figure(figsize=(16, 16))\n",
    "    plt.imshow(np.moveaxis(horizontal_grid.numpy(), 0, -1))\n",
    "    plt.axis('off')\n",
    "    if plot_horizontal:\n",
    "        display.display(plt.gcf())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breeding-ordering",
   "metadata": {},
   "source": [
    "## Generator and Discriminator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mechanical-semester",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DiscriminatorNet(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A three hidden-layer discriminative neural network\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DiscriminatorNet, self).__init__()\n",
    "        n_features = 3072\n",
    "        n_out = 1\n",
    "\n",
    "        self.hidden0 = nn.Sequential(\n",
    "            nn.Linear(n_features, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        self.hidden1 = nn.Sequential(\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        self.hidden2 = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        self.out = nn.Sequential(\n",
    "            torch.nn.Linear(256, n_out),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hidden0(x)\n",
    "        x = self.hidden1(x)\n",
    "        x = self.hidden2(x)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class GeneratorNet(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A three hidden-layer generative neural network\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(GeneratorNet, self).__init__()\n",
    "        n_features = 100\n",
    "        n_out = 3072\n",
    "\n",
    "        self.hidden0 = nn.Sequential(\n",
    "            nn.Linear(n_features, 256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.hidden1 = nn.Sequential(\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.hidden2 = nn.Sequential(\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(1024, n_out),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hidden0(x)\n",
    "        x = self.hidden1(x)\n",
    "        x = self.hidden2(x)\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "isolated-myanmar",
   "metadata": {},
   "source": [
    "## Train a GAN\n",
    "\n",
    "* Note 1: You can use the `log_images` function to plot the generated images.\n",
    "* Note 2: The training process is quite slow so I wouldn't expect to reach high levels of accuracy, because the notebook will remain open only for 15 mins. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "democratic-portfolio",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_discriminator(discriminator, optimizer, real_data, fake_data, loss):\n",
    "    N = real_data.size(0)\n",
    "    # Reset gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Train on Real Data\n",
    "    prediction_real = discriminator(real_data)\n",
    "    # Calculate error and backpropagate\n",
    "    error_real = loss(prediction_real, Variable(torch.ones(N, 1)))\n",
    "    error_real.backward()\n",
    "\n",
    "    # Train on Fake Data\n",
    "    prediction_fake = discriminator(fake_data)\n",
    "    # Calculate error and backpropagate\n",
    "    error_fake = loss(prediction_fake, Variable(torch.zeros(N, 1)))\n",
    "    error_fake.backward()\n",
    "\n",
    "    # Update weights with gradients\n",
    "    optimizer.step()\n",
    "\n",
    "    # Return error and predictions for real and fake inputs\n",
    "    return error_real + error_fake, prediction_real, prediction_fake\n",
    "\n",
    "\n",
    "def train_generator(discriminator, optimizer, fake_data, loss):\n",
    "    # Reset gradients\n",
    "    N = fake_data.size(0)  \n",
    "    \n",
    "    # Sample noise and generate fake data\n",
    "    optimizer.zero_grad()  \n",
    "    \n",
    "    # Calculate error and backpropagate\n",
    "    prediction = discriminator(fake_data)  \n",
    "    error = loss(prediction, Variable(torch.ones(N, 1)))\n",
    "    \n",
    "    # Update weights with gradients\n",
    "    error.backward()  \n",
    "    optimizer.step()  \n",
    "    \n",
    "    # Return error\n",
    "    return error\n",
    "\n",
    "\n",
    "def train():\n",
    "\n",
    "    # Models, optimizers and losses\n",
    "    discriminator = DiscriminatorNet()\n",
    "    generator = GeneratorNet()\n",
    "    loss_d = nn.BCELoss()\n",
    "    loss_g = nn.BCELoss()\n",
    "    d_optimizer = optim.Adam(discriminator.parameters(), 0.0002)\n",
    "    g_optimizer = optim.Adam(generator.parameters(), 0.0002)\n",
    "\n",
    "    data_loader= load_data()\n",
    "    \n",
    "    num_epochs=1\n",
    "    num_batches = len(data_loader)\n",
    "    \n",
    "    num_test_samples = 48\n",
    "    test_noise = noise(num_test_samples)\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for n_batch, data in enumerate(data_loader):\n",
    "\n",
    "            (real_batch, labels) = data\n",
    "            N = real_batch.size(0)\n",
    "\n",
    "            # 1. Train Discriminator\n",
    "            real_data = real_batch.view(real_batch.size(0), -1)\n",
    "\n",
    "            # Generate fake data and detach so gradients are not calculated for generator)\n",
    "            latent_space_data = noise(N)\n",
    "            fake_data = generator(latent_space_data).detach()\n",
    "\n",
    "            d_error, d_pred_real, d_pred_fake = train_discriminator(discriminator, d_optimizer, real_data,\n",
    "                                                                          fake_data,\n",
    "                                                                          loss_d)\n",
    "            # 2. Train Generator\n",
    "\n",
    "            # Generate fake data TO train Generator\n",
    "            latent_space_data = noise(N)\n",
    "            fake_data = generator(latent_space_data)\n",
    "            # Train G\n",
    "            g_error = train_generator(discriminator, g_optimizer, fake_data, loss_g)  # Log batch error\n",
    "\n",
    "            \n",
    "            if n_batch % 50 == 0:\n",
    "                print('Epoch: [{}/{}], Batch Num: [{}/{}]'.format(\n",
    "                            epoch, num_epochs, n_batch, num_batches)\n",
    "                    )\n",
    "                print('Discriminator Loss: {:.4f}, Generator Loss: {:.4f}'.format(d_error, g_error))\n",
    "                print('D(x): {:.4f}, D(G(z)): {:.4f}'.format(d_pred_real.mean(), d_pred_fake.mean()))\n",
    "                print('------------------------------')\n",
    "                \n",
    "              \n",
    "    \n",
    "    print('Training finished')\n",
    "    \n",
    "    #Generate images\n",
    "    test_images = vectors_to_images(generator(test_noise))\n",
    "    test_images = test_images.data\n",
    "    log_images(test_images,test_images.size()[0])\n",
    "    \n",
    "train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
